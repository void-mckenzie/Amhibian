{'epochs': 20, 'batch_size': 1, 'ft_shots': 15, 'lr': 0.001, 'wd': 3e-05, 'loss_type': 'iou', 'scheduler': None, 'scheduler_params': None, 'norm_type': 'instance', 'volumetric': False, 'max_images': None, 'data_regime': 'all'}
Starting fine-tuning
Save path None
/scratch2/mganesh/MetaMedSeg/MetaMedSeg/splits/spleen
Using iou
Creating data loaders for fine-tuning
Training on all train data
Loaded pre-trained meta-model
Epoch 0
Learning rate 0.001000
Traceback (most recent call last):
  File "experiment_runner.py", line 235, in <module>
    num_selections=data.get("num_selections"), save_pth=save_path)
  File "experiment_runner.py", line 127, in fine_tune
    data_regime=data_regime, save_pth=save_pth))
  File "experiment_runner.py", line 95, in fine_tune_one_selection
    trainer.train_and_evaluate_all()
  File "/scratch2/mganesh/MetaMedSeg/MetaMedSeg/meta_learning/fine_tuning.py", line 245, in train_and_evaluate_all
    model = self.train(train_loader, val_loader, evaluate_every, name)
  File "/scratch2/mganesh/MetaMedSeg/MetaMedSeg/meta_learning/fine_tuning.py", line 166, in train
    optimizer.step()
  File "/home1/mganesh/miniconda3/envs/medseg/lib/python3.7/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home1/mganesh/miniconda3/envs/medseg/lib/python3.7/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home1/mganesh/miniconda3/envs/medseg/lib/python3.7/site-packages/torch/optim/adam.py", line 252, in step
    found_inf=found_inf)
  File "/home1/mganesh/miniconda3/envs/medseg/lib/python3.7/site-packages/torch/optim/adam.py", line 316, in adam
    found_inf=found_inf)
  File "/home1/mganesh/miniconda3/envs/medseg/lib/python3.7/site-packages/torch/optim/adam.py", line 410, in _single_tensor_adam
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
KeyboardInterrupt